


#include <iostream>
#include <mpi.h>
#include <cstdlib>
#include <ctime>
using namespace std;
int main(int argc, char** argv) {
    MPI_Init(&argc, &argv); // Инициализация MPI
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size); // Получение общего числа процессов
    int sum;
    if (world_size != 2) {
        cout << "2 processes!" << std::endl;
        MPI_Abort(MPI_COMM_WORLD, 1); // Аварийное завершение программы, если число процессов не равно 2 }
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); // Получение номера текущего процесса
    int numbers_count;
    if (world_rank == 0) {
        cout << "Kol-vo: ";
        cin >> numbers_count;}
    MPI_Bcast(&numbers_count, 1, MPI_INT, 0, MPI_COMM_WORLD); // Распределение количества чисел всем процессам
    MPI_Barrier(MPI_COMM_WORLD); // Синхронизация процессов перед началом замера времени
    double start_time = MPI_Wtime(); // Замер времени начала выполнения программы
    if (world_rank == 0) {
        // Процесс 0 генерирует случайные числа и отправляет их процессу 1
        srand(static_cast<unsigned>(time(nullptr)));
        int* numbers = new int[numbers_count]; // Выделение динамической памяти под массив чисел
        // Генерация случайных чисел
        for (int i = 0; i < numbers_count; ++i) {
            numbers[i] = rand() % 100;  // Генерация случайных чисел от 0 до 99
            cout << "Chislo " << i + 1 << " = " << numbers[i] << endl;
        }
        // Отправка чисел процессу 1
        MPI_Send(numbers, numbers_count, MPI_INT, 1, 0, MPI_COMM_WORLD);
        // Получение результата от процесса 1
        int sum;
        MPI_Recv(&sum, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        // Вывод результата
        cout << "Sum: " << sum << endl;
        delete[] numbers; // Освобождение динамической памяти
    }
    else if (world_rank == 1) {
        // Процесс 1 получает числа от процесса 0, суммирует их и отправляет результат обратно
        int* received_numbers = new int[numbers_count]; // Выделение динамической памяти под массив чисел
        // Получение чисел от процесса 0
        MPI_Recv(received_numbers, numbers_count, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        // Вычисление суммы
        sum = 0;
        for (int i = 0; i < numbers_count; ++i) {
            sum += received_numbers[i];
        }
        delete[] received_numbers; // Освобождение динамической памяти
        // Отправка результата процессу 0
        MPI_Send(&sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
    }
    MPI_Barrier(MPI_COMM_WORLD); // Синхронизация процессов перед завершением замера времени
    double end_time = MPI_Wtime(); // Замер времени окончания выполнения программы
    if (world_rank == 0) {
        // Вывод времени выполнения на экран только из процесса 0
        cout << "Time: " << end_time - start_time << " seconds" << endl;
    }
    MPI_Finalize(); // Завершение работы с MPI
    return 0;
}





//#include <iostream>
//#include <mpi.h>
//#include <vector>
//#include <cstdlib>
//#include <ctime>
//
//int main(int argc, char** argv) {
//    setlocale(LC_ALL, "Russian");
//    MPI_Init(&argc, &argv);
//
//    int world_size;
//    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
//
//    if (world_size != 2) {
//        std::cerr << "This program is designed to run with 2 processes." << std::endl;
//        MPI_Abort(MPI_COMM_WORLD, 1);
//    }
//
//    int world_rank;
//    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
//
//    const int numbers_count = 5;  // Количество случайных чисел для генерации
//
//    if (world_rank == 0) {
//        // Процесс 0 генерирует случайные числа и отправляет их процессу 1
//        std::vector<int> numbers(numbers_count);
//
//        // Генерация случайных чисел
//        srand(static_cast<unsigned>(time(nullptr)));
//        for (int i = 0; i < numbers_count; ++i) {
//            numbers[i] = rand() % 100;  // Генерация случайных чисел от 0 до 99
//        }
//
//        // Отправка чисел процессу 1
//        MPI_Send(numbers.data(), numbers_count, MPI_INT, 1, 0, MPI_COMM_WORLD);
//
//        // Получение результата от процесса 1
//        int sum;
//        MPI_Recv(&sum, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
//
//        // Вывод результата
//        std::cout << "Процесс 0: Получена сумма от процесса 1: " << sum << std::endl;
//    }
//    else if (world_rank == 1) {
//        // Процесс 1 получает числа от процесса 0, суммирует их и отправляет результат обратно
//        std::vector<int> received_numbers(numbers_count);
//
//        // Получение чисел от процесса 0
//        MPI_Recv(received_numbers.data(), numbers_count, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
//
//        // Вычисление суммы
//        int sum = 0;
//        for (int num : received_numbers) {
//            sum += num;
//        }
//
//        // Отправка результата процессу 0
//        MPI_Send(&sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
//    }
//
//    MPI_Finalize();
//
//    return 0;
//}
//
//
//
//
//
//
//
//
//
//
//				//Синхронное
//
//
////#include <iostream>
////#include <mpi.h>
////#include <cstdlib>
////#include <ctime>
////using namespace std;
////int main(int argc, char** argv) {
////    MPI_Init(&argc, &argv);
////    setlocale(LC_ALL, "Russian");
////    int world_size;
////    MPI_Comm_size(MPI_COMM_WORLD, &world_size);
////
////    if (world_size != 2) {
////        std::cerr << "This program is designed to run with 2 processes." << std::endl;
////        //MPI_Abort(MPI_COMM_WORLD, 1);
////        MPI_Finalize();
////        return 0;
////    }
////
////    int world_rank;
////    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
////
////    const int numbers_count = 5;  // Количество случайных чисел для генерации
////
////    if (world_rank == 0) {
////        // Процесс 0 генерирует случайные числа и отправляет их процессу 1
////        srand(static_cast<unsigned>(time(nullptr)));
////        int* numbers = new int[numbers_count];
////
////        // Генерация случайных чисел
////        for (int i = 0; i < numbers_count; ++i) {
////            numbers[i] = rand() % 100;  // Генерация случайных чисел от 0 до 99
////            cout << numbers[i] << endl;
////        }
////
////        // Отправка чисел процессу 1
////        MPI_Send(numbers, numbers_count, MPI_INT, 1, 0, MPI_COMM_WORLD);
////
////        // Получение результата от процесса 1
////        int sum;
////        MPI_Recv(&sum, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
////
////        // Вывод результата
////        cout << "Процесс 0: Получена сумма от процесса 1: " << sum << endl;
////
////        delete[] numbers;
////    }
////    else if (world_rank == 1) {
////        // Процесс 1 получает числа от процесса 0, суммирует их и отправляет результат обратно
////        int* received_numbers = new int[numbers_count];
////
////        // Получение чисел от процесса 0
////        MPI_Recv(received_numbers, numbers_count, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
////
////        // Вычисление суммы
////        int sum = 0;
////        for (int i = 0; i < numbers_count; ++i) {
////            sum += received_numbers[i];
////        }
////
////        if (world_rank == 0) {
////            // Вывод результата на экран только из процесса 0
////            std::cout << "Процесс 0: Получена сумма от процесса 1: " << sum << std::endl;
////        }
////
////        // Отправка результата процессу 0
////        MPI_Send(&sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
////
////        delete[] received_numbers;
////    }
////
////    MPI_Finalize();
////
////    return 0;
////}
//
//
//
//
//
//				//Асинхронное
//
//
//#include <iostream>
//#include <mpi.h>
//#include <cstdlib>
//#include <ctime>
//using namespace std;
//int main(int argc, char** argv) {
//    MPI_Init(&argc, &argv); // Инициализация MPI
//
//    int world_size;
//    MPI_Comm_size(MPI_COMM_WORLD, &world_size); // Получение общего числа процессов
//
//    if (world_size != 2) {
//        std::cerr << "This program is designed to run with 2 processes." << std::endl;
//        MPI_Abort(MPI_COMM_WORLD, 1); // Аварийное завершение программы, если число процессов не равно 2
//    }
//
//    int world_rank;
//    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); // Получение номера текущего процесса
//
//    const int numbers_count = 5;  // Количество случайных чисел для генерации
//
//    if (world_rank == 0) {
//        // Процесс 0 генерирует случайные числа и отправляет их процессу 1
//        srand(static_cast<unsigned>(time(nullptr)));
//        int* numbers = new int[numbers_count]; // Выделение динамической памяти под массив чисел
//
//        // Генерация случайных чисел
//        for (int i = 0; i < numbers_count; ++i) {
//            numbers[i] = rand() % 100;  // Генерация случайных чисел от 0 до 99
//        }
//
//        MPI_Request send_request;
//        // Асинхронная отправка чисел процессу 1
//        MPI_Isend(numbers, numbers_count, MPI_INT, 1, 0, MPI_COMM_WORLD, &send_request);
//
//        // Дополнительный код, который может выполняться параллельно с отправкой
//
//        MPI_Wait(&send_request, MPI_STATUS_IGNORE); // Ожидание завершения асинхронной отправки
//
//        delete[] numbers; // Освобождение динамической памяти
//    }
//    else if (world_rank == 1) {
//        // Процесс 1 получает числа от процесса 0, суммирует их и отправляет результат обратно
//        int* received_numbers = new int[numbers_count]; // Выделение динамической памяти под массив чисел
//
//        MPI_Request recv_request;
//        // Асинхронное получение чисел от процесса 0
//        MPI_Irecv(received_numbers, numbers_count, MPI_INT, 0, 0, MPI_COMM_WORLD, &recv_request);
//
//        // Дополнительный код, который может выполняться параллельно с получением
//
//        MPI_Wait(&recv_request, MPI_STATUS_IGNORE); // Ожидание завершения асинхронного приема
//
//        // Вычисление суммы
//        int sum = 0;
//        for (int i = 0; i < numbers_count; ++i) {
//            sum += received_numbers[i];
//        }
//        
//        cout << "Процесс 0: Получена сумма от процесса 1: " << sum << endl;
//        
//        delete[] received_numbers; // Освобождение динамической памяти
//
//        // Отправка результата процессу 0
//        MPI_Send(&sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);
//    }
//
//
//    MPI_Finalize(); // Завершение работы с MPI
//
//    return 0;
//}
//
//
//
//
//
//
//            //Коллективное
//
//
////#include <iostream>
////#include <mpi.h>
////#include <cstdlib>
////#include <ctime>
////
////int main(int argc, char** argv) {
////    MPI_Init(&argc, &argv); // Инициализация MPI
////
////    int world_size;
////    MPI_Comm_size(MPI_COMM_WORLD, &world_size); // Получение общего числа процессов
////
////    if (world_size != 2) {
////        std::cerr << "This program is designed to run with 2 processes." << std::endl;
////        MPI_Abort(MPI_COMM_WORLD, 1); // Аварийное завершение программы, если число процессов не равно 2
////    }
////
////    int world_rank;
////    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); // Получение номера текущего процесса
////
////    const int numbers_count = 5;  // Количество случайных чисел для генерации
////
////    int* numbers = new int[numbers_count]; // Выделение динамической памяти под массив чисел
////
////    // Генерация случайных чисел
////    srand(static_cast<unsigned>(time(nullptr)));
////    for (int i = 0; i < numbers_count; ++i) {
////        numbers[i] = rand() % 100;  // Генерация случайных чисел от 0 до 99
////    }
////
////    // Вычисление суммы чисел
////    int sum = 0;
////    for (int i = 0; i < numbers_count; ++i) {
////        sum += numbers[i];
////    }
////
////    int global_sum;
////
////    // Суммирование результатов всех процессов и сохранение в переменной global_sum
////    MPI_Reduce(&sum, &global_sum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
////
////    if (world_rank == 0) {
////        // Вывод результата на экран только из процесса 0
////        std::cout << "Процесс 0: Получена сумма от процесса 1: " << global_sum << std::endl;
////    }
////
////    delete[] numbers; // Освобождение динамической памяти
////
////    MPI_Finalize(); // Завершение работы с MPI
////
////    return 0;
////}
//
////Здесь используется MPI_Reduce для суммирования результатов всех процессов и 
//// получения общей суммы в процессе с рангом 0. Однако, в данном контексте это 
//// может быть избыточным и менее эффективным, чем прямой обмен сообщениями.
//
//







//#2 #2



//#include <iostream>
//#include <cstdlib>
//#include <ctime>
//#include <mpi.h>
//using namespace std;
//#define N 1000  // Размер вектора
//
//int main(int argc, char** argv) {
//    MPI_Init(&argc, &argv);
//
//    int proc_rank, proc_count;
//    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
//    MPI_Comm_size(MPI_COMM_WORLD, &proc_count);
//
//    // Вычисление размера блока для каждого процесса
//    int block_size = N / proc_count;
//    int remainder = N % proc_count;
//
//    // Корректировка размера блока для остатка
//    int my_block_size = (proc_rank < remainder) ? block_size + 1 : block_size;
//    int my_start = proc_rank * block_size + ((proc_rank < remainder) ? proc_rank : remainder);
//
//    // Выделение памяти для локального вектора
//    int* local_vector = new int[my_block_size];
//
//    // Инициализация локального вектора случайными числами
//    srand(time(nullptr) + proc_rank); // Инициализация генератора случайных чисел с использованием номера процесса
//    for (int i = 0; i < my_block_size; i++) {
//        local_vector[i] = rand() % 100;  // Генерация случайного числа в диапазоне от 0 до 99
//    }
//
//    // Вычисление локальной суммы
//    int local_sum = 0;
//    for (int i = 0; i < my_block_size; i++) {
//        local_sum += local_vector[i];
//    }
//    cout << local_sum << endl;
//
//    // Сбор всех локальных сумм на корневой процесс (с рангом 0)
//    int* all_sums = nullptr;
//    if (proc_rank == 0) {
//        all_sums = new int[proc_count];
//    }
//
//    MPI_Gather(&local_sum, 1, MPI_INT, all_sums, 1, MPI_INT, 0, MPI_COMM_WORLD);
//
//    // Вывод результатов на корневом процессе
//    if (proc_rank == 0) {
//        // Вычисление глобальной суммы
//        int global_sum = 0;
//        for (int i = 0; i < proc_count; i++) {
//            global_sum += all_sums[i];
//        }
//
//        // Вывод результатов на экран
//        std::cout << "Процессор " << proc_rank << " из " << proc_count
//            << ": Частичная сумма = " << local_sum << ", Глобальная сумма = " << global_sum << std::endl;
//
//        // Освобождение памяти
//        delete[] all_sums;
//    }
//
//    // Освобождение локальной памяти
//    delete[] local_vector;
//
//    MPI_Finalize();
//    return 0;
//}







//#2 #3

//#include <iostream>
//#include <cstdlib>
//#include <ctime>
//#include <mpi.h>
//using namespace std;
//
//#define N 1000  // Размер вектора
//
//int main(int argc, char** argv) {
//    MPI_Init(&argc, &argv);
//
//    int proc_rank, proc_count;
//    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
//    MPI_Comm_size(MPI_COMM_WORLD, &proc_count);
//
//    // Вычисление размера блока для каждого процесса
//    int block_size = N / proc_count;
//    int remainder = N % proc_count;
//
//    // Вычисление начального индекса и размера блока для циклического распределения
//    int my_start = proc_rank;
//    int my_block_size = N / proc_count + ((proc_rank < remainder) ? 1 : 0);
//
//    // Выделение памяти для локального вектора
//    int* local_vector = new int[my_block_size];
//
//    // Инициализация локального вектора случайными числами
//    srand(time(nullptr) + proc_rank);
//    for (int i = 0; i < my_block_size; i++) {
//        // Генерация случайного числа в диапазоне от 0 до 99
//        local_vector[i] = rand() % 100;
//    }
//
//    // Вычисление локальной суммы
//    int local_sum = 0;
//    for (int i = 0; i < my_block_size; i++) {
//        local_sum += local_vector[i];
//    }
//    cout << local_sum << endl;
//
//    // Сбор всех локальных сумм на корневой процесс (с рангом 0)
//    int* all_sums = nullptr;
//    if (proc_rank == 0) {
//        all_sums = new int[proc_count];
//    }
//
//    MPI_Gather(&local_sum, 1, MPI_INT, all_sums, 1, MPI_INT, 0, MPI_COMM_WORLD);
//
//    // Вывод результатов на корневом процессе
//    if (proc_rank == 0) {
//        // Вычисление глобальной суммы
//        int global_sum = 0;
//        for (int i = 0; i < proc_count; i++) {
//            global_sum += all_sums[i];
//        }
//
//        // Вывод результатов на экран
//        std::cout << "Процессор " << proc_rank << " из " << proc_count
//            << ": Частичная сумма = " << local_sum << ", Глобальная сумма = " << global_sum << std::endl;
//
//        // Освобождение памяти
//        delete[] all_sums;
//    }
//
//    // Освобождение локальной памяти
//    delete[] local_vector;
//
//    MPI_Finalize();
//    return 0;
//}
//
////Этот код распределяет элементы вектора с использованием циклического распределения по процессам. Он использует my_start для указания начального индекса каждого процесса и my_block_size для указания размера блока.





//#3 #2
//#include <iostream>
//#include <cstdlib>
//#include <ctime>
//#include <mpi.h>
//
//using namespace std;
//
//#define N 1000  // Размер вектора
//
//int main(int argc, char** argv) {
//    MPI_Init(&argc, &argv);
//
//    int proc_rank, proc_count;
//    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
//    MPI_Comm_size(MPI_COMM_WORLD, &proc_count);
//
//    // Вычисление размера блока для каждого процесса
//    int block_size = N / proc_count;
//    int remainder = N % proc_count;
//
//    // Корректировка размера блока для остатка
//    int my_block_size = (proc_rank < remainder) ? block_size + 1 : block_size;
//    int my_start = proc_rank * block_size + ((proc_rank < remainder) ? proc_rank : remainder);
//
//    // Выделение памяти для локального вектора
//    int* local_vector = new int[my_block_size];
//
//    // Инициализация локального вектора случайными числами
//    srand(time(nullptr) + proc_rank);
//    for (int i = 0; i < my_block_size; i++) {
//        local_vector[i] = rand() % 100;  // Генерация случайного числа в диапазоне от 0 до 99
//    }
//
//    // Вычисление локальной суммы
//    int local_sum = 0;
//    for (int i = 0; i < my_block_size; i++) {
//        local_sum += local_vector[i];
//    }
//
//    // Сбор всех локальных сумм на процессе с рангом 0
//    int* all_sums = nullptr;
//    if (proc_rank == 0) {
//        all_sums = new int[proc_count];
//    }
//
//    MPI_Gather(&local_sum, 1, MPI_INT, all_sums, 1, MPI_INT, 0, MPI_COMM_WORLD);
//
//    // Вывод результатов
//    cout << "Процессор " << proc_rank << " из " << proc_count
//        << ": Частичная сумма = " << local_sum << endl;
//
//    // Вывод глобальной суммы на процессе с рангом 0
//    if (proc_rank == 0) {
//        int global_sum = 0;
//        for (int i = 0; i < proc_count; i++) {
//            global_sum += all_sums[i];
//        }
//
//        cout << "Глобальная сумма = " << global_sum << endl;
//
//        // Освобождение памяти
//        delete[] all_sums;
//    }
//
//    // Освобождение локальной памяти
//    delete[] local_vector;
//
//    MPI_Finalize();
//    return 0;
//}




//#3 #4

//#include <iostream>
//#include <cstdlib>
//#include <ctime>
//#include <mpi.h>
//
//using namespace std;
//
//#define N 1000000  // Размер вектора
//
//int main(int argc, char** argv) {
//    MPI_Init(&argc, &argv);
//
//    int proc_rank, proc_count;
//    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
//    MPI_Comm_size(MPI_COMM_WORLD, &proc_count);
//
//    double start_time, end_time;
//
//    // Блочное распределение
//    MPI_Barrier(MPI_COMM_WORLD);
//    start_time = MPI_Wtime();
//
//    int block_size = N / proc_count;
//    int remainder = N % proc_count;
//    int my_block_size = (proc_rank < remainder) ? block_size + 1 : block_size;
//    int my_start = proc_rank * block_size + ((proc_rank < remainder) ? proc_rank : remainder);
//
//    int* local_vector_block = new int[my_block_size];
//    srand(time(nullptr) + proc_rank);
//    for (int i = 0; i < my_block_size; i++) {
//        local_vector_block[i] = rand() % 100;
//    }
//
//    int local_sum_block = 0;
//    for (int i = 0; i < my_block_size; i++) {
//        local_sum_block += local_vector_block[i];
//    }
//
//    int global_sum_block = 0;
//    MPI_Reduce(&local_sum_block, &global_sum_block, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
//
//    end_time = MPI_Wtime();
//    if (proc_rank == 0) {
//        cout << "Блочное распределение: Глобальная сумма = " << global_sum_block << ", Время = " << end_time - start_time << " сек" << endl;
//    }
//
//    delete[] local_vector_block;
//
//    // Циклическое распределение
//    MPI_Barrier(MPI_COMM_WORLD);
//    start_time = MPI_Wtime();
//
//    int my_start_cycle = proc_rank;
//    int my_block_size_cycle = N / proc_count + ((proc_rank < remainder) ? 1 : 0);
//
//    int* local_vector_cycle = new int[my_block_size_cycle];
//    srand(time(nullptr) + proc_rank);
//    for (int i = 0; i < my_block_size_cycle; i++) {
//        local_vector_cycle[i] = rand() % 100;
//    }
//
//    int local_sum_cycle = 0;
//    for (int i = 0; i < my_block_size_cycle; i++) {
//        local_sum_cycle += local_vector_cycle[i];
//    }
//
//    int global_sum_cycle = 0;
//    MPI_Reduce(&local_sum_cycle, &global_sum_cycle, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);
//
//    end_time = MPI_Wtime();
//    if (proc_rank == 0) {
//        cout << "Циклическое распределение: Глобальная сумма = " << global_sum_cycle << ", Время = " << end_time - start_time << " сек" << endl;
//    }
//
//    delete[] local_vector_cycle;
//
//    MPI_Finalize();
//    return 0;
//}





//#4 #2

//#include <iostream>
//#include <cstdlib>
//#include <ctime>
//#include <mpi.h>
//
//using namespace std;
//
//#define N 1000000  // Размер вектора
//
//int main(int argc, char** argv) {
//    MPI_Init(&argc, &argv);
//
//    int proc_rank, proc_count;
//    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);
//    MPI_Comm_size(MPI_COMM_WORLD, &proc_count);
//
//    double start_time, end_time;
//
//    // Блочное распределение с использованием асинхронных операций MPI_Waitall
//    MPI_Barrier(MPI_COMM_WORLD);
//    start_time = MPI_Wtime();
//
//    int block_size = N / proc_count;
//    int remainder = N % proc_count;
//    int my_block_size = (proc_rank < remainder) ? block_size + 1 : block_size;
//    int my_start = proc_rank * block_size + ((proc_rank < remainder) ? proc_rank : remainder);
//
//    int* local_vector_block = new int[my_block_size];
//    int* recv_buffer = new int[proc_count];
//    MPI_Request* send_requests = new MPI_Request[proc_count];
//    MPI_Request recv_request;
//
//    // Инициализация локального вектора случайными числами
//    srand(time(nullptr) + proc_rank);
//    for (int i = 0; i < my_block_size; i++) {
//        local_vector_block[i] = rand() % 100;
//    }
//
//    // Асинхронная отправка локальных векторов
//    for (int i = 0; i < proc_count; i++) {
//        MPI_Isend(local_vector_block, my_block_size, MPI_INT, i, 0, MPI_COMM_WORLD, &send_requests[i]);
//    }
//
//    // Асинхронное приемное операций
//    MPI_Irecv(recv_buffer, proc_count, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &recv_request);
//
//    // Ожидание завершения всех асинхронных операций отправки
//    MPI_Waitall(proc_count, send_requests, MPI_STATUSES_IGNORE);
//
//    // Ожидание завершения асинхронной операции приема
//    MPI_Wait(&recv_request, MPI_STATUS_IGNORE);
//
//    // Обработка принятых данных
//    int global_sum_block = 0;
//    for (int i = 0; i < proc_count; i++) {
//        global_sum_block += recv_buffer[i];
//    }
//
//    end_time = MPI_Wtime();
//    if (proc_rank == 0) {
//        cout << "Блочное распределение с MPI_Waitall: Глобальная сумма = " << global_sum_block << ", Время = " << end_time - start_time << " сек" << endl;
//    }
//
//    delete[] local_vector_block;
//    delete[] recv_buffer;
//    delete[] send_requests;
//
//    MPI_Finalize();
//    return 0;
//}

