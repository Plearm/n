import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import line_search

# Определение функции и её градиента
def f(x):
    x1, x2 = x
    return 6 * x1**2 + x2**2 - x1 * x2 + x1

def gradient(x):
    x1, x2 = x
    df_dx1 = 12 * x1 - x2 + 1
    df_dx2 = 2 * x2 - x1
    return np.array([df_dx1, df_dx2])

# Метод наискорейшего спуска
def steepest_descent(f, grad, x0, constraint, learning_rate=0.01, tol=1e-6, max_iter=1000):
    x = x0
    steps = [x0]
    for i in range(max_iter):
        grad_val = grad(x)
        alpha = line_search(f, grad, x, -grad_val)[0]
        if alpha is None:
            alpha = learning_rate
        x_new = x - alpha * grad_val
        # Проекция на плоскость ограничений
        x_new = projection(x_new, constraint)
        steps.append(x_new)
        if np.linalg.norm(x_new - x) < tol:
            break
        x = x_new
    return x, steps

# Функция проекции на плоскость ограничений
def projection(x, constraint):
    A, b = constraint
    return x - np.dot(A, x - b) * A.T / np.dot(A, A)

# Параметры
x0 = np.array([2.0, 1.0])
constraint = (np.array([2.0, 3.0]), np.array([1.0]))

# Нахождение точки минимума методом наискорейшего спуска
xmin_sd, steps_sd = steepest_descent(f, gradient, x0, constraint)

# Значение функции в точке минимума
fmin_sd = f(xmin_sd)

# Вывод шагов
print("Шаги методом наискорейшего спуска:")
for i, step in enumerate(steps_sd):
    print(f"Step {i}: x = ({step[0]:.5f}, {step[1]:.5f}), f(x) = {f(step):.5f}")

print(f"\nТочка минимума: x = ({xmin_sd[0]:.5f}, {xmin_sd[1]:.5f})")
print(f"Значение функции в точке минимума: f(x) = {fmin_sd:.5f}")

# Сходимость
iterations_sd = len(steps_sd)
print(f"\nКоличество итераций: {iterations_sd}")

# График
x = np.linspace(-1, 3, 400)
y = np.linspace(-1, 2, 400)
X, Y = np.meshgrid(x, y)
Z = f([X, Y])

plt.figure(figsize=(12, 6))
cp = plt.contourf(X, Y, Z, levels=50, cmap='viridis')
plt.colorbar(cp)
plt.plot([s[0] for s in steps_sd], [s[1] for s in steps_sd], 'ro-', label='Steepest Descent Path')
plt.title('Steepest Descent Method')
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.grid(True)
plt.show()




















import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize_scalar

def f(x):
    return 6 * x[0]**2 + x[1]**2 - x[0] * x[1] + x[0]

def gradient(x):
    df_dx1 = 12 * x[0] - x[1] + 1
    df_dx2 = 2 * x[1] - x[0]
    return np.array([df_dx1, df_dx2])

def steepest_descent(f, grad, x0, epsilon):
    x = x0
    steps = [x0]
    while True:
        grad_x = grad(x)
        if np.linalg.norm(grad_x) < epsilon:
            break
        direction = -grad_x
        alpha = minimize_scalar(lambda alpha: f(x + alpha * direction)).x
        x_new = x + alpha * direction
        steps.append(x_new)
        x = x_new
    return x, steps

x0 = np.array([2, 1])
epsilon = 0.5

xmin, steps = steepest_descent(f, gradient, x0, epsilon)

print(f"Точка минимума: {xmin}")
print(f"Значение функции в точке минимума: {f(xmin)}")
print("Метод сходится" if np.linalg.norm(gradient(xmin)) < epsilon else "Метод не сходится")

# Построение графика
steps = np.array(steps)
plt.plot(steps[:, 0], steps[:, 1], 'bo-', label='Steps')
plt.scatter(xmin[0], xmin[1], color='red', label='Minimum')
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.title('Метод наискорейшего спуска')
plt.grid(True)
plt.show()

